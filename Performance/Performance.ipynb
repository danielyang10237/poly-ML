{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset, load_metric, DatasetDict\n",
    "import csv\n",
    "import sentencepiece\n",
    "import google.protobuf\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dyang/miniconda3/envs/NLP_env3.8/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:562: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"Customer Service\",\n",
    "    \"Audio\",\n",
    "    \"Visual\",\n",
    "    \"Bluetooth\",\n",
    "    \"Microphone\",\n",
    "    \"Battery Life\",\n",
    "    \"Internet\",\n",
    "    \"Performance/Speed\",\n",
    "    \"Value/Price\",\n",
    "    \"Ease of Use\",\n",
    "    \"Comfortability/Fit\",\n",
    "    \"Undetermined\"\n",
    "]\n",
    "class2id = {class_:id for id, class_ in enumerate(classes)}\n",
    "id2class = {id:class_ for id, class_ in enumerate(classes)}\n",
    "model_path = \"microsoft/deberta-v3-large\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the database\n",
    "with open(\"reviews_labeled.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18687\n"
     ]
    }
   ],
   "source": [
    "bad_data = [0]\n",
    "def preprocess_function(example):\n",
    "    if 'Review' not in example or 'Label' not in example:\n",
    "        bad_data[0] += 1\n",
    "        print(f\"Bad data: {bad_data[0]}\")\n",
    "        return None\n",
    "\n",
    "    text = example['Review']\n",
    "    label = example['Label']\n",
    "\n",
    "    labels = [0.0 for _ in classes]\n",
    "    if label in class2id:\n",
    "        labels[class2id[label]] = 1.0\n",
    "\n",
    "    tokenized_sentence = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "    return {\n",
    "        \"input_ids\": tokenized_sentence['input_ids'],  # Adjusted to how your tokenizer returns values\n",
    "        \"attention_mask\": tokenized_sentence['attention_mask'],\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "dataset_arr = []\n",
    "for row in data:\n",
    "    if len(row) > 1 and row[1] in classes:\n",
    "        processed = preprocess_function({\"Review\": row[0], \"Label\": row[1]})\n",
    "        if processed:\n",
    "            dataset_arr.append(processed)\n",
    "\n",
    "print(len(dataset_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 14949\n",
      "Validation: 1869\n",
      "Test: 1869\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into training, validation, and testing\n",
    "train_data, test_data = train_test_split(dataset_arr, test_size=0.2, random_state=42)\n",
    "test_data, val_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_data)}\")\n",
    "print(f\"Validation: {len(val_data)}\")\n",
    "print(f\"Test: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "val_data_dict = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "test_data_dict = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "\n",
    "for data in train_data:\n",
    "    train_data_dict[\"input_ids\"].append(data[\"input_ids\"])\n",
    "    train_data_dict[\"attention_mask\"].append(data[\"attention_mask\"])\n",
    "    train_data_dict[\"labels\"].append(data[\"labels\"])\n",
    "\n",
    "for data in val_data:\n",
    "    val_data_dict[\"input_ids\"].append(data[\"input_ids\"])\n",
    "    val_data_dict[\"attention_mask\"].append(data[\"attention_mask\"])\n",
    "    val_data_dict[\"labels\"].append(data[\"labels\"])\n",
    "\n",
    "for data in test_data:\n",
    "    test_data_dict[\"input_ids\"].append(data[\"input_ids\"])\n",
    "    test_data_dict[\"attention_mask\"].append(data[\"attention_mask\"])\n",
    "    test_data_dict[\"labels\"].append(data[\"labels\"])\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_data_dict)\n",
    "val_dataset = Dataset.from_dict(val_data_dict)\n",
    "test_dataset = Dataset.from_dict(test_data_dict)\n",
    "\n",
    "tokenized_datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple function to convert lists to tensors\n",
    "def collate_fn(batch):\n",
    "    return {key: torch.tensor([d[key] for d in batch]) for key in batch[0]}\n",
    "\n",
    "# Creating DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, collate_fn=collate_fn, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load the multi class classification model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=len(classes), id2label=id2class, label2id=class2id, problem_type=\"multi_label_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_over_time = []\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, optimizer, training_args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(f\"Training on {device}\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(training_args.num_train_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_progress_bar = tqdm(train_dataloader, total=len(train_dataloader),\n",
    "                                  desc=f\"Epoch {epoch + 1}/{training_args.num_train_epochs} Training\")\n",
    "\n",
    "        for i, batch in enumerate(train_progress_bar):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            if (i + 1) % 40 == 0:\n",
    "                print(f\"Epoch {epoch + 1}, Batch {i + 1}, Loss: {loss.item():.4f}\")\n",
    "            if i < 1000 and i % 50 == 0:\n",
    "                loss_over_time.append(loss.item())\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if training_args.evaluation_strategy == 'epoch':\n",
    "            model.eval()\n",
    "            total_val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(val_dataloader, desc=\"Validating\"):\n",
    "                    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "                    labels = batch['labels'].to(device)\n",
    "                    outputs = model(**inputs)\n",
    "                    val_loss = criterion(outputs.logits, labels)\n",
    "                    total_val_loss += val_loss.item()\n",
    "\n",
    "            avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "            print(f\"Epoch {epoch + 1}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                if training_args.save_strategy == \"best\":\n",
    "                    torch.save(model.state_dict(), os.path.join(training_args.output_dir, 'best_model.pth'))\n",
    "\n",
    "        if training_args.save_strategy == \"epoch\":\n",
    "            torch.save(model.state_dict(), os.path.join(training_args.output_dir, f'model_epoch_{epoch + 1}.pth'))\n",
    "\n",
    "    if training_args.load_best_model_at_end and training_args.save_strategy == \"best\":\n",
    "        model.load_state_dict(torch.load(os.path.join(training_args.output_dir, 'best_model.pth')))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dyang/miniconda3/envs/NLP_env3.8/lib/python3.8/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:   2%|â–         | 40/1869 [00:14<11:01,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 40, Loss: 0.0029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:   4%|â–         | 80/1869 [00:29<11:06,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 80, Loss: 0.0056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:   6%|â–‹         | 120/1869 [00:44<10:33,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 120, Loss: 0.0190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:   9%|â–Š         | 160/1869 [00:59<10:08,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 160, Loss: 0.0264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  11%|â–ˆ         | 200/1869 [01:14<10:38,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 200, Loss: 0.0062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  13%|â–ˆâ–Ž        | 240/1869 [01:29<10:18,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 240, Loss: 0.0232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  15%|â–ˆâ–        | 280/1869 [01:43<09:40,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 280, Loss: 0.0567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  17%|â–ˆâ–‹        | 320/1869 [01:58<09:17,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 320, Loss: 0.7280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  19%|â–ˆâ–‰        | 360/1869 [02:13<09:41,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 360, Loss: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  21%|â–ˆâ–ˆâ–       | 400/1869 [02:28<09:16,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 400, Loss: 0.0318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  24%|â–ˆâ–ˆâ–Ž       | 440/1869 [02:43<08:40,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 440, Loss: 0.3226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  26%|â–ˆâ–ˆâ–Œ       | 480/1869 [02:58<08:51,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 480, Loss: 0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  28%|â–ˆâ–ˆâ–Š       | 520/1869 [03:13<08:15,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 520, Loss: 0.2586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  30%|â–ˆâ–ˆâ–‰       | 560/1869 [03:27<08:21,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 560, Loss: 0.9909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 600/1869 [03:42<07:46,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 600, Loss: 0.6650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 640/1869 [03:57<07:33,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 640, Loss: 0.2316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 680/1869 [04:12<07:16,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 680, Loss: 0.0169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 720/1869 [04:27<06:48,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 720, Loss: 0.0744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 760/1869 [04:42<07:05,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 760, Loss: 0.6293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 800/1869 [04:56<06:47,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 800, Loss: 0.1553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 840/1869 [05:11<06:23,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 840, Loss: 0.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 880/1869 [05:26<06:07,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 880, Loss: 1.0264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 920/1869 [05:41<05:58,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 920, Loss: 0.8588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 960/1869 [05:56<05:45,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 960, Loss: 0.0909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1000/1869 [06:11<05:28,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1000, Loss: 0.2568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1040/1869 [06:25<04:59,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1040, Loss: 0.1526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1080/1869 [06:40<05:00,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1080, Loss: 0.6467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1120/1869 [06:55<04:36,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1120, Loss: 0.3118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1160/1869 [07:10<04:32,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1160, Loss: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1200/1869 [07:25<04:01,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1200, Loss: 0.1098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1240/1869 [07:40<03:44,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1240, Loss: 0.0107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1280/1869 [07:55<03:51,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1280, Loss: 0.4342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1320/1869 [08:10<03:30,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1320, Loss: 0.2093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1360/1869 [08:25<03:10,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1360, Loss: 0.0073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1400/1869 [08:40<02:50,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1400, Loss: 0.0231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1440/1869 [08:55<02:33,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1440, Loss: 0.2952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1480/1869 [09:10<02:23,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1480, Loss: 0.4042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1520/1869 [09:25<02:16,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1520, Loss: 0.0639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1560/1869 [09:39<01:49,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1560, Loss: 0.0431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1600/1869 [09:54<01:40,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1600, Loss: 0.0925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1640/1869 [10:09<01:23,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1640, Loss: 0.1810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1680/1869 [10:24<01:12,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1680, Loss: 0.0670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1720/1869 [10:39<00:55,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1720, Loss: 0.1662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1760/1869 [10:54<00:39,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1760, Loss: 0.1855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1800/1869 [11:09<00:25,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1800, Loss: 0.2902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1840/1869 [11:24<00:11,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1840, Loss: 0.1161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1869/1869 [11:34<00:00,  2.69it/s]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 234/234 [00:23<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 0.4782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:   2%|â–         | 40/1869 [00:14<10:54,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 40, Loss: 0.0396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:   4%|â–         | 80/1869 [00:29<11:03,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 80, Loss: 0.0312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:   6%|â–‹         | 120/1869 [00:44<11:18,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 120, Loss: 0.0422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:   9%|â–Š         | 160/1869 [00:59<10:55,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 160, Loss: 0.0177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  11%|â–ˆ         | 200/1869 [01:13<10:21,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 200, Loss: 0.4184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  13%|â–ˆâ–Ž        | 240/1869 [01:28<09:58,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 240, Loss: 0.0247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  15%|â–ˆâ–        | 280/1869 [01:43<09:28,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 280, Loss: 0.0528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  17%|â–ˆâ–‹        | 320/1869 [01:58<09:40,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 320, Loss: 0.0561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  19%|â–ˆâ–‰        | 360/1869 [02:12<09:07,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 360, Loss: 0.0124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  21%|â–ˆâ–ˆâ–       | 400/1869 [02:27<09:02,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 400, Loss: 0.0422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  24%|â–ˆâ–ˆâ–Ž       | 440/1869 [02:42<09:02,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 440, Loss: 0.0366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  26%|â–ˆâ–ˆâ–Œ       | 480/1869 [02:57<08:28,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 480, Loss: 0.0291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  28%|â–ˆâ–ˆâ–Š       | 520/1869 [03:12<09:40,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 520, Loss: 0.0268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  30%|â–ˆâ–ˆâ–‰       | 560/1869 [03:27<08:15,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 560, Loss: 0.1825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 600/1869 [03:42<07:42,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 600, Loss: 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 640/1869 [03:57<07:46,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 640, Loss: 0.0465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 680/1869 [04:11<07:23,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 680, Loss: 0.1315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 720/1869 [04:26<07:15,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 720, Loss: 0.0270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 760/1869 [04:41<06:51,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 760, Loss: 0.0646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 800/1869 [04:56<06:27,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 800, Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 840/1869 [05:11<06:28,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 840, Loss: 0.0141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 880/1869 [05:25<05:53,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 880, Loss: 0.0197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 920/1869 [05:40<05:34,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 920, Loss: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 960/1869 [05:55<05:41,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 960, Loss: 0.2928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1000/1869 [06:10<05:27,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 1000, Loss: 0.0128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1040/1869 [06:25<04:50,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 1040, Loss: 0.0211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1080/1869 [06:40<04:43,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 1080, Loss: 0.1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1120/1869 [06:55<04:29,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 1120, Loss: 0.3651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1160/1869 [07:09<04:28,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 1160, Loss: 0.1130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1200/1869 [07:24<03:56,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 1200, Loss: 0.0230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1240/1869 [07:39<03:51,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 1240, Loss: 0.0146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1280/1869 [07:54<03:32,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 1280, Loss: 0.2081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1320/1869 [08:08<03:17,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 1320, Loss: 0.0728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1360/1869 [08:23<03:17,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 1360, Loss: 0.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1400/1869 [08:38<02:57,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 1400, Loss: 0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1440/1869 [08:53<02:47,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 1440, Loss: 0.0433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1480/1869 [09:07<02:19,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 1480, Loss: 0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1520/1869 [09:22<02:02,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 1520, Loss: 0.6267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1560/1869 [09:37<01:55,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 1560, Loss: 0.0050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1600/1869 [09:52<01:39,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 1600, Loss: 0.3137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1640/1869 [10:07<01:24,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 1640, Loss: 0.0065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1680/1869 [10:22<01:10,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 1680, Loss: 0.0096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1720/1869 [10:36<00:53,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 1720, Loss: 0.0099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1760/1869 [10:51<00:39,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 1760, Loss: 0.0918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1800/1869 [11:06<00:24,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 1800, Loss: 0.0060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1840/1869 [11:21<00:10,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 1840, Loss: 0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1869/1869 [11:32<00:00,  2.70it/s]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 234/234 [00:23<00:00,  9.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Validation Loss: 0.4682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DebertaV2ForSequenceClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 1024, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=12, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "\n",
    "   output_dir=\"classification_beta\",\n",
    "   learning_rate=2e-4,\n",
    "   per_device_train_batch_size=3,\n",
    "   per_device_eval_batch_size=3,\n",
    "   num_train_epochs=2,\n",
    "   weight_decay=0.01,\n",
    "   evaluation_strategy=\"epoch\",\n",
    "   save_strategy=\"epoch\",\n",
    "   load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = train(model, train_dataloader, val_dataloader, optimizer, training_args)\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Testing:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 145/234 [00:14<00:08,  9.99it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 234/234 [00:23<00:00,  9.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss: 0.54\n",
      "Accuracy: 87.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=len(classes), id2label=id2class, label2id=class2id, problem_type=\"multi_label_classification\")\n",
    "model.load_state_dict(torch.load('classification_beta/model_epoch_2.pth'))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "total_test_loss = 0\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(**inputs)\n",
    "        test_loss = criterion(outputs.logits, labels)\n",
    "        total_test_loss += test_loss.item()\n",
    "\n",
    "        # Compute accuracy\n",
    "        predicted_labels = torch.max(outputs.logits, 1)[1]  # Gets the indices of max logit\n",
    "        predicted_labels_one_hot = F.one_hot(predicted_labels, num_classes=len(classes)).to(device)\n",
    "\n",
    "        # Ensure labels are in the correct dtype for comparison, they should be in the same dtype as 'predicted_labels_one_hot'\n",
    "        labels = labels.to(torch.int64)  # Adjust dtype if necessary\n",
    "\n",
    "        # Now calculate the accuracy\n",
    "        correct_predictions += (predicted_labels_one_hot == labels).all(dim=1).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "\n",
    "print(f\"Average test loss: {avg_test_loss:.2f}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Comfortability/Fit\n"
     ]
    }
   ],
   "source": [
    "# custom input into the model\n",
    "custom_text = \"The shape of the mouse is very uncomfortable, it makes my hand hurt\"\n",
    "\n",
    "tokenized_text = tokenizer(custom_text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "tokenized_text = {k: v.to(device) for k, v in tokenized_text.items()}\n",
    "output = model(**tokenized_text)\n",
    "\n",
    "predicted_labels = torch.max(output.logits, 1)[1]\n",
    "predicted_class = id2class[predicted_labels.item()]\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_env3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
