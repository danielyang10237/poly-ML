{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/dyang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/dyang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.utils import resample\n",
    "import nltk\n",
    "\n",
    "# Ensure required NLTK resources are available\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(integer):\n",
    "    if (integer == 'positive'):\n",
    "        return 1\n",
    "    elif (integer == 'negative'):\n",
    "        return -1\n",
    "    elif (integer == 'neutral'):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total datapoints:  200229\n",
      "number of positive sentiments:  88080\n",
      "number of neutral sentiments:  68355\n",
      "number of negative sentiments:  43787\n",
      "After upsampling:\n",
      "category\n",
      " 1.0    88080\n",
      " 0.0    88080\n",
      "-1.0    88080\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# collect our datapoints\n",
    "sentiment_reddit = pd.read_csv('sentiment_data/Reddit_Data.csv')\n",
    "sentiment_twitter = pd.read_csv('sentiment_data/Twitter_Data.csv')\n",
    "\n",
    "sarcasm = []\n",
    "with open(\"sarcasm_data/Sarcasm_Headlines_Dataset_v2.json\", 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        sarcasm.append(json.loads(line))\n",
    "sarcasm_v2 = pd.DataFrame(sarcasm)\n",
    "\n",
    "sarcasm = []\n",
    "with open (\"sarcasm_data/Sarcasm_Headlines_Dataset.json\", 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        sarcasm.append(json.loads(line))\n",
    "sarcasm_v1 = pd.DataFrame(sarcasm)\n",
    "\n",
    "# concactenate the datasets\n",
    "sentiments = pd.concat([sentiment_reddit, sentiment_twitter])\n",
    "sarcasm = pd.concat([sarcasm_v1, sarcasm_v2])\n",
    "\n",
    "print(\"total datapoints: \", sentiments.shape[0])\n",
    "print(\"number of positive sentiments: \", sentiments[sentiments['category'] == 1].shape[0])\n",
    "print(\"number of neutral sentiments: \", sentiments[sentiments['category'] == 0].shape[0])\n",
    "print(\"number of negative sentiments: \", sentiments[sentiments['category'] == -1].shape[0])\n",
    "\n",
    "data_majority = sentiments[sentiments['category'] == 1]\n",
    "data_minority = sentiments[sentiments['category'] == 0]\n",
    "data_minority2 = sentiments[sentiments['category'] == -1]\n",
    "\n",
    "data_minority_upsampled = resample(data_minority,\n",
    "                                   replace=True,\n",
    "                                   n_samples=data_majority.shape[0], \n",
    "                                   random_state=8)\n",
    "\n",
    "data_minority2_upsampled = resample(data_minority2,\n",
    "                                    replace=True,\n",
    "                                    n_samples=data_majority.shape[0],\n",
    "                                    random_state=8)\n",
    "\n",
    "sentiments_upsampled = pd.concat([data_majority, data_minority_upsampled, data_minority2_upsampled])\n",
    "\n",
    "print(\"After upsampling:\")\n",
    "print(sentiments_upsampled['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total datapoints:  55328\n",
      "number of sarcastic headlines:  25358\n",
      "number of non-sarcastic headlines:  29970\n",
      "After upsampling:\n",
      "is_sarcastic\n",
      "0    29970\n",
      "1    29970\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Now we also need to upsample the sarcasm dataset\n",
    "\n",
    "print(\"total datapoints: \", sarcasm.shape[0])\n",
    "print(\"number of sarcastic headlines: \", sarcasm[sarcasm['is_sarcastic'] == 1].shape[0])\n",
    "print(\"number of non-sarcastic headlines: \", sarcasm[sarcasm['is_sarcastic'] == 0].shape[0])\n",
    "\n",
    "data_majority = sarcasm[sarcasm['is_sarcastic'] == 0]\n",
    "data_minority = sarcasm[sarcasm['is_sarcastic'] == 1]\n",
    "\n",
    "\n",
    "data_minority_upsampled = resample(data_minority,\n",
    "                                      replace=True,\n",
    "                                      n_samples=data_majority.shape[0],\n",
    "                                      random_state=8) \n",
    "\n",
    "sarcasm_upsampled = pd.concat([data_majority, data_minority_upsampled])\n",
    "\n",
    "print(\"After upsampling:\")\n",
    "print(sarcasm_upsampled['is_sarcastic'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def depure_data(data):\n",
    "    if pd.isna(data):\n",
    "        return \"\"  # Return empty string for missing values\n",
    "    data = str(data)  # Convert data to string to ensure compatibility with regex\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    data = url_pattern.sub('', data)\n",
    "    data = re.sub('\\S*@\\S*\\s?', '', data)\n",
    "    data = re.sub('\\s+', ' ', data)\n",
    "    data = re.sub(\"'\", \"\", data)\n",
    "    return data\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield gensim.utils.simple_preprocess(str(sentence), deacc=True)\n",
    "\n",
    "def preprocess(data_frame, column_name='clean_comment', remove_stopwords=True, lemmatize=True):\n",
    "    temp = data_frame[column_name].apply(depure_data).tolist()\n",
    "    data_words = list(sent_to_words(temp))\n",
    "    if remove_stopwords:\n",
    "        stop_words = gensim.parsing.preprocessing.STOPWORDS\n",
    "        data_words = [[word for word in doc if word not in stop_words] for doc in data_words]\n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        data_words = [[lemmatizer.lemmatize(word) for word in doc] for doc in data_words]\n",
    "    data = [TreebankWordDetokenizer().detokenize(words) for words in data_words]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['clean_comment', 'category', 'clean_text'], dtype='object')\n",
      "Index(['article_link', 'headline', 'is_sarcastic'], dtype='object')\n",
      "                                        article_link  \\\n",
      "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
      "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
      "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
      "5  https://www.huffingtonpost.com/entry/advancing...   \n",
      "6  https://www.huffingtonpost.com/entry/how-meat-...   \n",
      "\n",
      "                                            headline  is_sarcastic  \n",
      "0  former versace store clerk sues over secret 'b...             0  \n",
      "1  the 'roseanne' revival catches up to our thorn...             0  \n",
      "4  j.k. rowling wishes snape happy birthday in th...             0  \n",
      "5                        advancing the world's women             0  \n",
      "6     the fascinating case for eating lab-grown meat             0  \n"
     ]
    }
   ],
   "source": [
    "print(sentiments_upsampled.columns)\n",
    "print(sarcasm_upsampled.columns)\n",
    "\n",
    "print(sarcasm_upsampled.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_data = preprocess(sentiments_upsampled)\n",
    "sarcasm_data = preprocess(sarcasm_upsampled, column_name='headline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was teens when discovered zen meditation was then undiagnosed bpd being homeschooled and just gotten 56k modem with web connection where came across link zen meditation tried for couple weeks and the change was palpable felt the most profound sense peace ever felt grades immediately started going had more energy started martial arts just huge positive change all around parents asked something was wrong fundie parents when anything changes this was where naivetÃ© kicked foolishly told them been trying meditation and really calmed down thought they happy that found something that helped but never forget what happened next mother affected this mockingly calm breathy voice she said you can pretend calm and happy all you want but without jesus you never content was that moment that any belief had christian faith all vanished completely realized that she had probably never been happy ever have never felt profoundly sorry for someone did for her that moment \n",
      "advancing the world's women\n",
      "advancing world woman\n",
      "teen discovered zen meditation undiagnosed bpd homeschooled gotten modem web connection came link zen meditation tried couple week change palpable felt profound sense peace felt grade immediately started going energy started martial art huge positive change parent asked wrong fundie parent change naivete kicked foolishly told trying meditation calmed thought happy helped forget happened mother affected mockingly calm breathy voice said pretend calm happy want jesus content moment belief christian faith vanished completely realized probably happy felt profoundly sorry moment\n",
      "59940\n",
      "264240\n"
     ]
    }
   ],
   "source": [
    "# before\n",
    "print(sentiments_upsampled['clean_comment'].iloc[3])\n",
    "print(sarcasm_upsampled['headline'].iloc[3])\n",
    "# after\n",
    "print(sarcasm_data[3])\n",
    "print(sentiment_data[3])\n",
    "\n",
    "# lengths\n",
    "print(len(sarcasm_data))\n",
    "print(len(sentiment_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dyang/miniconda3/envs/NLP_env3.8/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the text data into tokenized data (numerical data)\n",
    "max_len = 120\n",
    "max_words =tokenizer.vocab_size\n",
    "\n",
    "assert isinstance(sentiment_data, list), \"sentiment_data should be a list of strings\"\n",
    "assert isinstance(sarcasm_data, list), \"sarcasm_data should be a list of strings\"\n",
    "\n",
    "data_sentiment = sentiment_data\n",
    "tokenized_sentiment = tokenizer(data_sentiment, padding=True, truncation=True, max_length=max_len, return_tensors='pt')\n",
    "\n",
    "data_sarcasm = sarcasm_data\n",
    "tokenized_sarcasm = tokenizer(data_sarcasm, padding=True, truncation=True, max_length=max_len, return_tensors='pt')\n",
    "\n",
    "assert tokenized_sentiment['input_ids'].shape[1] == max_len\n",
    "assert tokenized_sarcasm['input_ids'].shape[1] == max_len, f\"Expected {max_len} but got {tokenized_sarcasm['input_ids'].shape[1]} for sarcasm data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0,  ..., 1, 1, 1]) 59940\n",
      "tensor([2., 2., 2.,  ..., 0., 0., 0.]) 264240\n"
     ]
    }
   ],
   "source": [
    "# Make the label tensors\n",
    "adjusted_labels_sentiment = [label + 1 for label in sentiments_upsampled['category']]\n",
    "labels_sentiments = torch.tensor(adjusted_labels_sentiment)\n",
    "\n",
    "adjusted_labels_sarcasm = [label for label in sarcasm_upsampled['is_sarcastic']]\n",
    "labels_sarcasm = torch.tensor(adjusted_labels_sarcasm)\n",
    "\n",
    "print(labels_sarcasm, len(labels_sarcasm))\n",
    "print(labels_sentiments, len(labels_sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([264240, 120])\n",
      "59940\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_sentiment['input_ids'].shape)\n",
    "print(len(labels_sarcasm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198180 66060 198180 66060\n",
      "47952 11988 47952 11988\n",
      "torch.Size([47952, 120]) torch.Size([47952, 120]) torch.Size([47952])\n"
     ]
    }
   ],
   "source": [
    "X_train_sentiment_ids, X_test_sentiment_ids, X_train_sentiment_mask, X_test_sentiment_mask, y_train_sentiment, y_test_sentiment = train_test_split(\n",
    "    tokenized_sentiment['input_ids'], tokenized_sentiment['attention_mask'], labels_sentiments, test_size=0.25)\n",
    "\n",
    "X_train_sarcasm_ids, X_test_sarcasm_ids, X_train_sarcasm_mask, X_test_sarcasm_mask, y_train_sarcasm, y_test_sarcasm = train_test_split(\n",
    "    tokenized_sarcasm['input_ids'], tokenized_sarcasm['attention_mask'], labels_sarcasm, test_size=0.2)\n",
    "\n",
    "print(len(X_train_sentiment_ids), len(X_test_sentiment_ids), len(y_train_sentiment), len(y_test_sentiment))\n",
    "print(len(X_train_sarcasm_ids), len(X_test_sarcasm_ids), len(y_train_sarcasm), len(y_test_sarcasm))\n",
    "print(X_train_sarcasm_ids.shape, X_train_sarcasm_mask.shape, y_train_sarcasm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# import the pytorch dependencies\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"using device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criteria_sentiment = nn.CrossEntropyLoss()\n",
    "criteria_sarcasm = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentSarcasmModel(nn.Module):\n",
    "    def __init__(self, nb_words, embedding_dim):\n",
    "        super(SentimentSarcasmModel, self).__init__()\n",
    "\n",
    "        # Make our embedding layers\n",
    "        self.embedding1 = nn.Embedding(nb_words, embedding_dim)\n",
    "        self.embedding1.weight.requires_grad = True\n",
    "        self.embedding2 = nn.Embedding(nb_words, embedding_dim)\n",
    "        self.embedding2.weight.requires_grad = True\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, embedding_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(embedding_dim, embedding_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Sentiment inner branch hidden layers\n",
    "        self.fc1_1 = nn.Linear(embedding_dim * 2, 256)  # Changed from 20 * 2 to 128 * 2\n",
    "        self.drop1_1 = nn.Dropout(0.4)\n",
    "        self.fc1_2 = nn.Linear(256, 32)\n",
    "        self.drop1_2 = nn.Dropout(0.4)\n",
    "        self.fc1_3 = nn.Linear(32, 3)\n",
    "\n",
    "        # Sarcasm inner branch hidden layers\n",
    "        self.fc2_1 = nn.Linear(embedding_dim * 2, 256)  # Changed from 20 * 2 to 128 * 2\n",
    "        self.drop2_1 = nn.Dropout(0.4)\n",
    "        self.fc2_2 = nn.Linear(256, 32)\n",
    "        self.drop2_2 = nn.Dropout(0.4)\n",
    "        self.fc2_3 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.embedding1(x1)\n",
    "        x2 = self.embedding2(x2)\n",
    "\n",
    "        x1, _ = self.lstm1(x1)\n",
    "        x2, _ = self.lstm2(x2)\n",
    "\n",
    "        # Use the output of the last time step for each sequence\n",
    "        x1 = x1[:, -1, :]\n",
    "        x2 = x2[:, -1, :]\n",
    "\n",
    "        x1 = F.relu(self.fc1_1(x1))\n",
    "        x1 = self.drop1_1(x1)\n",
    "        x1 = F.relu(self.fc1_2(x1))\n",
    "        x1 = self.drop1_2(x1)\n",
    "        x1 = self.fc1_3(x1)\n",
    "\n",
    "        x2 = F.relu(self.fc2_1(x2))\n",
    "        x2 = self.drop2_1(x2)\n",
    "        x2 = F.relu(self.fc2_2(x2))\n",
    "        x2 = self.drop2_2(x2)\n",
    "        x2 = self.fc2_3(x2)\n",
    "\n",
    "        return x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss Sentiment: 0.0344, Loss Sarcasm: 0.0218\n",
      "Epoch [2/10], Loss Sentiment: 0.0344, Loss Sarcasm: 0.0218\n",
      "Epoch [3/10], Loss Sentiment: 0.0344, Loss Sarcasm: 0.0218\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m batch_sentiment \u001b[38;5;241m=\u001b[39m X_train_sentiment_ids[batch_indices]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     30\u001b[0m labels_sentiment \u001b[38;5;241m=\u001b[39m y_train_sentiment[batch_indices]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 32\u001b[0m output_sentiment, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_sentiment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sarcasm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m labels_sentiment \u001b[38;5;241m=\u001b[39m labels_sentiment\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m     36\u001b[0m loss_sentiment \u001b[38;5;241m=\u001b[39m criteria_sentiment(output_sentiment, labels_sentiment)\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP_env3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP_env3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[75], line 34\u001b[0m, in \u001b[0;36mSentimentSarcasmModel.forward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     31\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding2(x2)\n\u001b[1;32m     33\u001b[0m x1, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm1(x1)\n\u001b[0;32m---> 34\u001b[0m x2, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Use the output of the last time step for each sequence\u001b[39;00m\n\u001b[1;32m     37\u001b[0m x1 \u001b[38;5;241m=\u001b[39m x1[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP_env3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP_env3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP_env3.8/lib/python3.8/site-packages/torch/nn/modules/rnn.py:878\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    875\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 878\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    882\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming model is already defined and device is set\n",
    "model = SentimentSarcasmModel(nb_words=max_words, embedding_dim=128).to(device)\n",
    "model.train()\n",
    "\n",
    "# Define number of epochs\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "Y_train_sentiment = y_train_sentiment.long()\n",
    "Y_train_sarcasm = y_train_sarcasm.long()\n",
    "\n",
    "num_samples_sentiment = X_train_sentiment_ids.size(0)\n",
    "indices_sentiment = torch.randperm(num_samples_sentiment)\n",
    "\n",
    "num_samples_sarcasm = X_train_sarcasm_ids.size(0)\n",
    "indices_sarcasm = torch.randperm(num_samples_sarcasm)\n",
    "\n",
    "prev_gradient = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss_sentiment = 0.0\n",
    "    total_loss_sarcasm = 0.0\n",
    "    \n",
    "    for i in range(0, num_samples_sentiment, batch_size):\n",
    "        batch_indices = indices_sentiment[i:i+batch_size]\n",
    "        batch_sentiment = X_train_sentiment_ids[batch_indices].to(device)\n",
    "        labels_sentiment = y_train_sentiment[batch_indices].to(device)\n",
    "\n",
    "        output_sentiment, _ = model(batch_sentiment, batch_sarcasm)\n",
    "\n",
    "        labels_sentiment = labels_sentiment.long()\n",
    "\n",
    "        loss_sentiment = criteria_sentiment(output_sentiment, labels_sentiment)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_sentiment.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss_sentiment += loss_sentiment.item()\n",
    "    \n",
    "    for i in range(0, num_samples_sarcasm, batch_size):\n",
    "        batch_indices = indices_sarcasm[i:i+batch_size]\n",
    "        batch_sarcasm = X_train_sarcasm_ids[batch_indices].to(device)\n",
    "        labels_sarcasm = y_train_sarcasm[batch_indices].to(device)\n",
    "\n",
    "        _, output_sarcasm = model(batch_sentiment, batch_sarcasm)\n",
    "\n",
    "        labels_sarcasm = labels_sarcasm.long()\n",
    "\n",
    "        loss_sarcasm = criteria_sarcasm(output_sarcasm, labels_sarcasm)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_sarcasm.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss_sarcasm += loss_sarcasm.item()\n",
    "\n",
    "    # Print average losses for the epoch\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss Sentiment: {total_loss_sentiment/num_samples_sentiment:.4f}, Loss Sarcasm: {total_loss_sarcasm/num_samples_sarcasm:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Sentiment: 0.3328, Accuracy Sarcasm: 0.5021\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "batch_size = 32\n",
    "num_samples_sentiment = X_test_sentiment_ids.size(0)\n",
    "num_samples_sarcasm = X_test_sarcasm_ids.size(0)\n",
    "\n",
    "# Initialize counters for correct predictions\n",
    "correct_sentiment = 0\n",
    "correct_sarcasm = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Iterate over the sentiment test data in batches\n",
    "    for i in range(0, num_samples_sentiment, batch_size):\n",
    "        batch_indices = slice(i, min(i + batch_size, num_samples_sentiment))\n",
    "        batch_sentiment = X_test_sentiment_ids[batch_indices].to(device)\n",
    "        labels_sentiment = y_test_sentiment[batch_indices].to(device)\n",
    "\n",
    "        output_sentiment, _ = model(batch_sentiment, torch.zeros_like(batch_sentiment))  # Assuming mock input for sarcasm\n",
    "        _, predicted_sentiment = torch.max(output_sentiment, 1)\n",
    "        correct_sentiment += (predicted_sentiment == labels_sentiment).sum().item()\n",
    "\n",
    "    # Iterate over the sarcasm test data in batches\n",
    "    for i in range(0, num_samples_sarcasm, batch_size):\n",
    "        batch_indices = slice(i, min(i + batch_size, num_samples_sarcasm))\n",
    "        batch_sarcasm = X_test_sarcasm_ids[batch_indices].to(device)\n",
    "        labels_sarcasm = y_test_sarcasm[batch_indices].to(device)\n",
    "\n",
    "        _, output_sarcasm = model(torch.zeros_like(batch_sarcasm), batch_sarcasm)  # Assuming mock input for sentiment\n",
    "        _, predicted_sarcasm = torch.max(output_sarcasm, 1)\n",
    "        correct_sarcasm += (predicted_sarcasm == labels_sarcasm).sum().item()\n",
    "\n",
    "# Calculate accuracies\n",
    "accuracy_sentiment = correct_sentiment / num_samples_sentiment\n",
    "accuracy_sarcasm = correct_sarcasm / num_samples_sarcasm\n",
    "\n",
    "print(f'Accuracy Sentiment: {accuracy_sentiment:.4f}, Accuracy Sarcasm: {accuracy_sarcasm:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
