{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from queue import Queue\n",
    "import re\n",
    "\n",
    "# spacy.require_gpu(0)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "FANBOYS = [\"for\", \"and\", \"nor\", \"but\", \"or\", \"yet\", \"so\"]\n",
    "FANBOYS = [\", \" + x for x in FANBOYS] + [\",\" + x for x in FANBOYS]\n",
    "\n",
    "class WordNode:\n",
    "    def __init__(self, word):\n",
    "        self.text = word.text\n",
    "        self.pos = word.pos_\n",
    "        self.idx = word.i\n",
    "        self.lbl = word.dep_\n",
    "        self.children = []\n",
    "        self.parent = None\n",
    "        self.obj_root = False\n",
    "    \n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)\n",
    "    \n",
    "    def remove_child(self, child):\n",
    "        if child in self.children:\n",
    "            self.children.remove(child)\n",
    "\n",
    "    def add_parent(self, parent):\n",
    "        self.parent = parent\n",
    "    \n",
    "    def print(self):\n",
    "        print(\"Node summary:\", self.text, [x.text for x in self.children], self.obj_root)\n",
    "\n",
    "    def is_children(self, word):\n",
    "        return word in self.children\n",
    "    \n",
    "    def set_object(self):\n",
    "        self.obj_root = True\n",
    "\n",
    "def segment_verbs_and_objects(input_string):\n",
    "\n",
    "    doc = nlp(input_string)\n",
    "\n",
    "    word_nodes = []\n",
    "    verbs = []\n",
    "\n",
    "    for token in doc:\n",
    "        new_node = WordNode(token)\n",
    "        word_nodes.append(new_node)\n",
    "\n",
    "        # ancestors = [t.text for t in token.ancestors]\n",
    "        # children = [t.text for t in token.children]\n",
    "        # print(token.text, \"\\t\", token.i, \"\\t\", \n",
    "        #     token.pos_, \"\\t\", token.dep_, \"\\t\", \n",
    "        #     ancestors, \"\\t\", children)\n",
    "    \n",
    "    for token in doc:\n",
    "        for child in token.children:\n",
    "            word_nodes[token.i].add_child(word_nodes[child.i])\n",
    "    \n",
    "    # for node in word_nodes:\n",
    "    #     node.print()\n",
    "    \n",
    "    # find the root verb of the sentence\n",
    "    def find_root_of_sentence(doc):\n",
    "        root_token = None\n",
    "        for token in doc:\n",
    "            if (token.dep_ == \"ROOT\" and token.pos_ == \"VERB\"):\n",
    "                root_token = token\n",
    "        return root_token\n",
    "    \n",
    "    root_token = find_root_of_sentence(doc)\n",
    "\n",
    "    if not root_token:\n",
    "        return [input_string]\n",
    "    \n",
    "    # find the subject of the sentence\n",
    "    def get_subject(root_token):\n",
    "        for child in root_token.children:\n",
    "            if (child.dep_ == \"nsubj\"):\n",
    "                return child\n",
    "        \n",
    "        return None\n",
    "\n",
    "    subject = get_subject(root_token)\n",
    "    if subject == None:\n",
    "        return [input_string]\n",
    "    \n",
    "    # route the root verb to the subject\n",
    "    word_nodes[root_token.i].add_parent(word_nodes[subject.i])\n",
    "    word_nodes[root_token.i].remove_child(word_nodes[subject.i])\n",
    "    word_nodes[subject.i].remove_child(word_nodes[root_token.i])\n",
    "    verbs.append(root_token)\n",
    "\n",
    "    # find any other verbs in the sentence\n",
    "    def parse_other_verbs(root_token):\n",
    "        other_verbs = []\n",
    "        for children in root_token.children:\n",
    "            if (children.pos_ == \"VERB\" and children != root_token):\n",
    "                other_verbs.append(children)\n",
    "                word_nodes[root_token.i].remove_child(word_nodes[children.i])\n",
    "                word_nodes[children.i].add_parent(word_nodes[subject.i])\n",
    "                other_verbs.extend(parse_other_verbs(children))\n",
    "            if (children.dep_ == \"nsubj\"):\n",
    "                word_nodes[root_token.i].add_parent(word_nodes[children.i])\n",
    "                word_nodes[root_token.i].remove_child(word_nodes[children.i])\n",
    "    \n",
    "        return other_verbs\n",
    "\n",
    "    verbs.extend(parse_other_verbs(root_token))\n",
    "\n",
    "    # now we find all dependent verbs (verbs without an object attached)\n",
    "    dependent_verbs = Queue()\n",
    "    for verb in verbs:\n",
    "        dependent = True\n",
    "        for child in verb.children:\n",
    "            if (child.dep_ == 'dobj' or child.pos_ == \"ADP\"):\n",
    "                dependent = False\n",
    "        if dependent:\n",
    "            dependent_verbs.put(verb)\n",
    "\n",
    "    # given a verb, find all its dependencies\n",
    "    visited = set()\n",
    "\n",
    "    def find_obj_dfs(verb_token, root_obj_token):\n",
    "        visited.add(root_obj_token.i)\n",
    "        object_nodes = []\n",
    "        for child in root_obj_token.children:\n",
    "            if (child.dep_ == \"conj\" or child.dep_ == 'dobj' or child.pos_ == \"ADP\") and child.i not in visited:\n",
    "                object_nodes.extend(find_obj_dfs(verb_token, child))\n",
    "                word_nodes[verb_token.i].add_child(word_nodes[child.i])\n",
    "                word_nodes[root_obj_token.i].remove_child(word_nodes[child.i])\n",
    "                word_nodes[child.i].set_object()\n",
    "                object_nodes.append(child)\n",
    "            if (child.dep_ == \"cc\" or child.dep_ == \"punct\"):\n",
    "                word_nodes[root_obj_token.i].remove_child(word_nodes[child.i])\n",
    "        \n",
    "        return object_nodes\n",
    "\n",
    "    def get_all_objects(verb_token):\n",
    "        object_nodes = []\n",
    "        for child in verb_token.children:\n",
    "            if (child.pos_ == \"NOUN\" or child.pos_ == \"PRON\" or child.pos_ == \"PROPN\" or child.pos_ == \"ADP\") and child.dep_ != \"nsubj\":\n",
    "                object_nodes.extend(find_obj_dfs(verb_token, child))\n",
    "                word_nodes[child.i].set_object()\n",
    "                object_nodes.append(child)\n",
    "            if (child.dep_ == \"cc\" or child.dep_ == \"punct\"):\n",
    "                word_nodes[verb_token.i].remove_child(word_nodes[child.i])\n",
    "        \n",
    "        return object_nodes\n",
    "    \n",
    "    # assign dependent verbs objects from the first verb that is not dependent\n",
    "    for action in verbs:\n",
    "        object_nodes = get_all_objects(action)\n",
    "        if len(object_nodes) > 0:\n",
    "            while not dependent_verbs.empty():\n",
    "                dependent_verb = dependent_verbs.get()\n",
    "                for object in object_nodes:\n",
    "                    word_nodes[dependent_verb.i].add_child(word_nodes[object.i])\n",
    "\n",
    "    # for node in word_nodes:\n",
    "    #     node.print()\n",
    "\n",
    "    # help to traverse the entire tree given a root node\n",
    "    def dfs_traverse(root_node):\n",
    "        seen_nodes = [root_node.idx]\n",
    "        for child in root_node.children:\n",
    "            more_nodes = dfs_traverse(child)\n",
    "            seen_nodes.extend(more_nodes)\n",
    "        return seen_nodes\n",
    "\n",
    "    # for each verb, we create its own clause\n",
    "    clauses = []\n",
    "    for action in verbs:\n",
    "        subjects = dfs_traverse(word_nodes[action.i].parent)\n",
    "        verb_extras = []\n",
    "\n",
    "        for child in word_nodes[action.i].children:\n",
    "            if child.obj_root == False:\n",
    "                verb_extras.extend(dfs_traverse(child))\n",
    "            \n",
    "                # print(\"verb_extras\", verb_extras, child.text)\n",
    "        \n",
    "        for child in word_nodes[action.i].children:\n",
    "            if child.obj_root == True:\n",
    "                obj = dfs_traverse(child)\n",
    "                total_sentence = subjects + verb_extras + obj + [action.i]\n",
    "                total_sentence.sort()\n",
    "                total_sentence_text = \"\"\n",
    "                for idx in total_sentence:\n",
    "                    total_sentence_text += word_nodes[idx].text + \" \"\n",
    "                clauses.append(total_sentence_text)\n",
    "                print(total_sentence_text)\n",
    "    \n",
    "    return clauses\n",
    "\n",
    "def segment_clauses(input_string):\n",
    "    pattern = '|'.join(map(re.escape, FANBOYS))\n",
    "    \n",
    "    clauses = re.split(pattern, input_string)\n",
    "    \n",
    "    for clause in clauses:\n",
    "        segment_verbs_and_objects(clause)\n",
    "\n",
    "\n",
    "def segment_sentences(input_string):\n",
    "    sentences = input_string.split(\".\")\n",
    "    for sentence in sentences:\n",
    "        segment_clauses(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mOriginal:\u001b[0m He loves and hates the mountains and the beach\n",
      "He loves the beach \n",
      "He loves the mountains \n",
      "He hates the mountains \n",
      "He hates the beach \n",
      "\u001b[31mOriginal:\u001b[0m The big brown fox loves guitar but smashes every guitar he sees\n",
      "The big brown fox loves guitar \n",
      "The big brown fox smashes every guitar he sees \n",
      "\u001b[31mOriginal:\u001b[0m I ran to the bench while he ran to the closet\n",
      "I ran to the bench \n",
      "while he ran to the closet \n",
      "\u001b[31mOriginal:\u001b[0m Sally and Mr Smith both love ice cream, popsicles and cake\n",
      "Sally and Mr Smith both love ice cream \n",
      "Sally and Mr Smith both love cake \n",
      "Sally and Mr Smith both love popsicles \n",
      "\u001b[31mOriginal:\u001b[0m I love the product but not the price\n",
      "I love the product \n",
      "I love not the price \n",
      "\u001b[31mOriginal:\u001b[0m I bought this product, which broke in two months, so I threw it away\n",
      "I bought this product which broke in two months \n",
      "I threw it away \n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_input = \"He loves and hates the mountains and the beach\"\n",
    "    print(\"\\033[31m\" + \"Original:\" + \"\\033[0m\", test_input)\n",
    "    segment_sentences(test_input)\n",
    "    test_input = \"The big brown fox loves guitar but smashes every guitar he sees\"\n",
    "    print(\"\\033[31m\" + \"Original:\" + \"\\033[0m\", test_input)\n",
    "    segment_sentences(test_input)\n",
    "    test_input = \"I ran to the bench while he ran to the closet\"\n",
    "    print(\"\\033[31m\" + \"Original:\" + \"\\033[0m\", test_input)\n",
    "    segment_sentences(test_input)\n",
    "    test_input = \"Sally and Mr Smith both love ice cream, popsicles and cake\"\n",
    "    print(\"\\033[31m\" + \"Original:\" + \"\\033[0m\", test_input)\n",
    "    segment_sentences(test_input)\n",
    "    test_input = \"I love the product but not the price\"\n",
    "    print(\"\\033[31m\" + \"Original:\" + \"\\033[0m\", test_input)\n",
    "    segment_sentences(test_input)\n",
    "    test_input = \"I bought this product, which broke in two months, so I threw it away\"\n",
    "    print(\"\\033[31m\" + \"Original:\" + \"\\033[0m\", test_input)\n",
    "    segment_sentences(test_input)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_env3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
