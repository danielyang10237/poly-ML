{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from queue import Queue\n",
    "import re\n",
    "import string\n",
    "\n",
    "# spacy.require_gpu(0)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "FANBOYS = [\"for\", \"and\", \"nor\", \"but\", \"or\", \"yet\", \"so\"]\n",
    "FANBOYS = [\", \" + x for x in FANBOYS] + [\",\" + x for x in FANBOYS]\n",
    "\n",
    "class WordNode:\n",
    "    def __init__(self, word):\n",
    "        self.text = word.text\n",
    "        self.pos = word.pos_\n",
    "        self.idx = word.i\n",
    "        self.lbl = word.dep_\n",
    "        self.children = []\n",
    "        self.parent = None\n",
    "        self.obj_root = False\n",
    "    \n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)\n",
    "    \n",
    "    def remove_child(self, child):\n",
    "        if child in self.children:\n",
    "            self.children.remove(child)\n",
    "\n",
    "    def add_parent(self, parent):\n",
    "        self.parent = parent\n",
    "    \n",
    "    def print(self):\n",
    "        print(\"Node summary:\", self.text, [x.text for x in self.children], self.obj_root)\n",
    "\n",
    "    def is_children(self, word):\n",
    "        return word in self.children\n",
    "    \n",
    "    def set_object(self):\n",
    "        self.obj_root = True\n",
    "\n",
    "    def unset_object(self):\n",
    "        self.obj_root = False\n",
    "\n",
    "# checks if word is punctuation\n",
    "def is_punctuation(word):\n",
    "    punct = [\".\", \",\", \"!\", \"?\", \":\", \";\", \"'\"]\n",
    "    for p in punct:\n",
    "        if p in word or word == ',':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# collects all the word indices based off our constructed tree\n",
    "def dfs_traverse(root_node):\n",
    "    seen_nodes = [root_node.idx]\n",
    "    obj_nodes = []\n",
    "    for child in root_node.children:\n",
    "        if child.obj_root == True:\n",
    "            obj_nodes.append(child)\n",
    "            continue\n",
    "        more_nodes, more_obj_nodes = dfs_traverse(child)\n",
    "        seen_nodes.extend(more_nodes)\n",
    "        obj_nodes.extend(more_obj_nodes)\n",
    "    return seen_nodes, obj_nodes\n",
    "\n",
    "# gets rid of all the attached coordinating conjunctions\n",
    "def strip_conjunctions(root_node):\n",
    "    for child in root_node.children:\n",
    "        if child.lbl == \"cc\":\n",
    "            root_node.remove_child(child)\n",
    "            for grandchild in child.children:\n",
    "                root_node.add_child(grandchild)\n",
    "        strip_conjunctions(child)\n",
    "\n",
    "def get_subject(input_sentence, orig_mapping = False):\n",
    "    doc = nlp(input_sentence)\n",
    "\n",
    "    word_nodes = []\n",
    "\n",
    "    # parsing the sentence from spacy\n",
    "    for token in doc:\n",
    "        new_node = WordNode(token)\n",
    "        word_nodes.append(new_node)\n",
    "\n",
    "        if orig_mapping:\n",
    "            ancestors = [t.text for t in token.ancestors]\n",
    "            children = [t.text for t in token.children]\n",
    "            print(token.text, \"\\t\", token.i, \"\\t\", \n",
    "                token.pos_, \"\\t\", token.dep_, \"\\t\", \n",
    "                ancestors, \"\\t\", children)\n",
    "    \n",
    "    # construct our own model of the tree\n",
    "    for token in doc:\n",
    "        for child in token.children:\n",
    "            word_nodes[token.i].add_child(word_nodes[child.i])\n",
    "    \n",
    "    # find the subject of the sentence\n",
    "    def get_subject():\n",
    "        for word in doc:\n",
    "            if (word.dep_ == \"nsubj\"):\n",
    "                return word\n",
    "        \n",
    "        return None\n",
    "\n",
    "    subject = get_subject()\n",
    "    \n",
    "    # find the root verb of the sentence\n",
    "    def find_root_of_sentence(doc):\n",
    "        root_token = None\n",
    "        for token in doc:\n",
    "            if (token.dep_ == \"ROOT\" and token.pos_ == \"VERB\"):\n",
    "                root_token = token\n",
    "        return root_token\n",
    "    \n",
    "    root_token = find_root_of_sentence(doc)\n",
    "\n",
    "    if not root_token:\n",
    "        return None, doc, word_nodes, root_token\n",
    "    \n",
    "    subject = word_nodes[subject.i]\n",
    "    root_token = word_nodes[root_token.i]\n",
    "\n",
    "    # route the root verb to the subject\n",
    "    root_token.add_parent(subject)\n",
    "    root_token.remove_child(subject)\n",
    "    subject.remove_child(root_token)\n",
    "\n",
    "    return subject, doc, word_nodes, root_token\n",
    "\n",
    "# give each verb its own sentence\n",
    "def segment_verbs(subject, word_nodes, root_token):\n",
    "    verb_sentences = []\n",
    "    verbs = [root_token]\n",
    "\n",
    "    # find any other verbs in the sentence\n",
    "    def parse_other_verbs(root_token):\n",
    "        other_verbs = []\n",
    "        for children in root_token.children:\n",
    "            if (children.pos == \"VERB\" and children != root_token):\n",
    "                other_verbs.append(children)\n",
    "                root_token.remove_child(children)\n",
    "                children.add_parent(subject)\n",
    "                other_verbs.extend(parse_other_verbs(children))\n",
    "                strip_conjunctions(root_token)\n",
    "            if (children.lbl == \"nsubj\"):\n",
    "                root_token.remove_child(children)\n",
    "                root_token.add_parent(children)\n",
    "    \n",
    "        return other_verbs\n",
    "\n",
    "    verbs.extend(parse_other_verbs(root_token))\n",
    "\n",
    "    # now we find all dependent verbs (verbs without an object attached)\n",
    "    dependent_verbs = Queue()\n",
    "    first_object = None\n",
    "    for verb in verbs:\n",
    "        dependent = True\n",
    "        for child in verb.children:\n",
    "            if (child.lbl == 'dobj' or child.pos == \"ADP\" or child.pos == \"NOUN\" or child.pos == \"PRON\"):\n",
    "                if first_object is None:\n",
    "                    first_object = child\n",
    "                dependent = False\n",
    "        if dependent:\n",
    "            dependent_verbs.put(verb)\n",
    "\n",
    "    if not first_object:\n",
    "        return []\n",
    "\n",
    "    # attach objects to all dependent verbs \n",
    "    while not dependent_verbs.empty():\n",
    "        dependent_verb = dependent_verbs.get()\n",
    "        dependent_verb.add_child(first_object)\n",
    "\n",
    "\n",
    "    # print(\"Segmented verbs\")\n",
    "\n",
    "    # for each verb, we create its own clause\n",
    "    for action in verbs:\n",
    "        subjects, _ = dfs_traverse(action.parent)\n",
    "        subjects.extend(dfs_traverse(action)[0])\n",
    "\n",
    "        total_sentence = sorted(subjects)\n",
    "        total_sentence_text = \"\"\n",
    "        for idx in total_sentence:\n",
    "            if total_sentence_text != \"\" and not is_punctuation(word_nodes[idx].text):\n",
    "                total_sentence_text += \" \"\n",
    "            total_sentence_text += word_nodes[idx].text\n",
    "        verb_sentences.append(total_sentence_text)\n",
    "\n",
    "        # print(total_sentence_text)\n",
    "    \n",
    "    return verb_sentences\n",
    "\n",
    "def segment_objects(verb_sentences, custom_mapping = False):\n",
    "    clauses = []\n",
    "\n",
    "    for sentence in verb_sentences:\n",
    "        _, _, word_nodes, root_token = get_subject(sentence)\n",
    "\n",
    "        if root_token == None:\n",
    "            print(\"could not find root token\")\n",
    "            continue\n",
    "\n",
    "        visited = set()\n",
    "\n",
    "        # checks for layered objects\n",
    "        def check_prepositional_phrases(parent):\n",
    "            count = 0\n",
    "            for child in parent.children:\n",
    "                if child.lbl == \"conj\" or child.lbl == \"dobj\" or child.lbl == \"pobj\":\n",
    "                    count += 1\n",
    "                # if child.lbl == \"prep\":\n",
    "                #     return True\n",
    "                # if child.pos == \"NOUN\" or child.lbl == \"dobj\":\n",
    "                #     for grandchild in child.children:\n",
    "                #         if (grandchild.lbl == \"conj\" or grandchild.pos == \"NOUN\" or grandchild.lbl == \"dobj\") and grandchild.lbl != \"compound\":\n",
    "                #             return True\n",
    "            return count <= 1\n",
    "\n",
    "        # map conjoining objects to their own branch\n",
    "        def dfs_down_rewire(grandparent, parent):\n",
    "            if parent.idx in visited:\n",
    "                return\n",
    "            visited.add(parent.idx)\n",
    "\n",
    "            for child in parent.children:\n",
    "                next_grandparent, next_parent = parent, child\n",
    "\n",
    "                # print(\"CHILD\", child.text, child.pos, child.lbl)\n",
    "                # print(\"PARENT\", parent.text, parent.pos, parent.lbl)\n",
    "\n",
    "                if (child.lbl == \"conj\" or child.lbl == \"dobj\" or child.lbl == \"pobj\") and child.lbl != \"compound\":\n",
    "                    if (parent.lbl == \"conj\" or parent.lbl == \"dobj\" or parent.lbl == \"pobj\") and child.lbl != \"compound\":\n",
    "                        grandparent.add_child(child)\n",
    "                        parent.remove_child(child)\n",
    "                        child.set_object()\n",
    "                        parent.set_object()\n",
    "                        strip_conjunctions(parent)\n",
    "\n",
    "                        next_grandparent = grandparent\n",
    "                \n",
    "                dfs_down_rewire(next_grandparent, next_parent)\n",
    "        \n",
    "        if check_prepositional_phrases(root_token):\n",
    "            for child in root_token.children:\n",
    "                dfs_down_rewire(root_token, child)\n",
    "        else:\n",
    "            for child in root_token.children:\n",
    "                if (child.lbl == \"conj\" or child.pos == \"NOUN\") and child.lbl != \"compound\":\n",
    "                    child.set_object()\n",
    "                    strip_conjunctions(root_token)\n",
    "\n",
    "        total_sentence_idx = dfs_traverse(root_token.parent)[0]\n",
    "        sentence_idxs, objects = dfs_traverse(root_token)\n",
    "        total_sentence_idx.extend(sentence_idxs)\n",
    "\n",
    "        # print(\"segmented objects\")\n",
    "\n",
    "        # form all the sentences with unique objects\n",
    "        if len(objects) == 0:\n",
    "            total_sentence_idx = sorted(total_sentence_idx)\n",
    "            total_sentence_text = \"\"\n",
    "            for idx in total_sentence_idx:\n",
    "                total_sentence_text += word_nodes[idx].text + \" \"\n",
    "            clauses.append(total_sentence_text)\n",
    "\n",
    "            # print(total_sentence_text)\n",
    "        else:\n",
    "            for obj in objects:\n",
    "                total_sentence = total_sentence_idx + dfs_traverse(obj)[0]\n",
    "                total_sentence.sort()\n",
    "                total_sentence_text = \"\"\n",
    "                for idx in total_sentence:\n",
    "                    if total_sentence_text != \"\" and not is_punctuation(word_nodes[idx].text):\n",
    "                        total_sentence_text += \" \"\n",
    "                    total_sentence_text += word_nodes[idx].text\n",
    "                clauses.append(total_sentence_text)\n",
    "\n",
    "                # print(total_sentence_text)\n",
    "        \n",
    "        if custom_mapping:\n",
    "            for node in word_nodes:\n",
    "                node.print()\n",
    "    \n",
    "    return clauses\n",
    "\n",
    "\n",
    "def segment_verbs_and_objects(input_string, orig_mapping = False, custom_mapping = False):\n",
    "\n",
    "    subject, doc, word_nodes, root_token = get_subject(input_string, orig_mapping)\n",
    "    \n",
    "    if subject == None:\n",
    "        return [input_string]\n",
    "    \n",
    "    # split the sentences with each sentence having one verb from the subject\n",
    "    verb_segmented_sentences = segment_verbs(subject, word_nodes, root_token)\n",
    "    if len(verb_segmented_sentences) == 0:\n",
    "        return [input_string]\n",
    "    \n",
    "    # split the sentences according to unique objects\n",
    "    clauses = segment_objects(verb_segmented_sentences, custom_mapping)\n",
    "    \n",
    "    return clauses\n",
    "\n",
    "def segment_clauses(input_string, orig_mapping = False, custom_mapping = False):\n",
    "    # pattern = '|'.join(map(re.escape, FANBOYS))\n",
    "    \n",
    "    # clauses = re.split(pattern, input_string)\n",
    "    \n",
    "    # for clause in clauses:\n",
    "    segmented = segment_verbs_and_objects(input_string, orig_mapping, custom_mapping)\n",
    "\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    all_words = (segment.split(\" \") for segment in segmented)\n",
    "    seen = set()\n",
    "\n",
    "    for words in all_words:\n",
    "        for word in words:\n",
    "            cleaned_word = word.translate(translator)\n",
    "            cleaned_word = word.replace(\"'s\", \"\")\n",
    "            seen.add(cleaned_word)\n",
    "    \n",
    "    doc = nlp(input_string)\n",
    "    for token in doc:\n",
    "        if (token.pos_ == \"NOUN\" or token.pos_ == \"PRON\" or token.pos_ == \"VERB\") and token.text not in seen:\n",
    "            print(token.text)\n",
    "            return [input_string]\n",
    "    return segmented\n",
    "\n",
    "def segment_sentences(input_string, orig_mapping = False, custom_mapping = False):\n",
    "    mini_sentences = []\n",
    "    sentences = input_string.split(\".\")\n",
    "    for sentence in sentences:\n",
    "        mini_sentences.extend(segment_clauses(sentence, orig_mapping, custom_mapping))\n",
    "    return mini_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mOriginal:\u001b[0m The product could use a little work on the sound and comfort\n",
      "['The product could use a little work on the sound', 'The product could use a little work on comfort']\n",
      "\u001b[31mOriginal:\u001b[0m I ran to the bench while he ran to the closet\n",
      "['I ran to the bench ', 'while he ran to the closet ']\n",
      "\u001b[31mOriginal:\u001b[0m He loves and hates the mountains and the beach\n",
      "['He loves the mountains', 'He loves the beach', 'He hates the mountains', 'He hates the beach']\n",
      "\u001b[31mOriginal:\u001b[0m The big brown fox loves guitar but smashes every guitar he sees\n",
      "['The big brown fox loves guitar ', 'The big brown fox smashes every guitar he sees ']\n",
      "\u001b[31mOriginal:\u001b[0m Sally and Mr Smith both love ice cream, popsicles and cake \n",
      "cream\n",
      "['Sally and Mr Smith both love ice cream, popsicles and cake ']\n",
      "\u001b[31mOriginal:\u001b[0m I love the product but not the price\n",
      "['I love the product', 'I love not the price']\n",
      "\u001b[31mOriginal:\u001b[0m I bought this product, which broke in two months, so I threw it away\n",
      "[', so I threw it away ', 'I bought this product , which broke in two months ']\n",
      "\u001b[31mOriginal:\u001b[0m I love this product's fit but not the color\n",
      "[\"I love this product's fit\", 'I love not the color']\n",
      "\u001b[31mOriginal:\u001b[0m The turtle won the race, and the rabbit lost his keys\n",
      "['The turtle won the race , ', 'the rabbit lost his keys ']\n"
     ]
    }
   ],
   "source": [
    "# More Complex Sentences\n",
    "if __name__ == \"__main__\":\n",
    "    test_input = \"The product could use a little work on the sound and comfort\"\n",
    "    print(\"\\033[31m\" + \"Original:\" + \"\\033[0m\", test_input)\n",
    "    print(segment_sentences(test_input, False, False))\n",
    "    test_input = \"I ran to the bench while he ran to the closet\"\n",
    "    print(\"\\033[31m\" + \"Original:\" + \"\\033[0m\", test_input)\n",
    "    print(segment_sentences(test_input, False, False))\n",
    "    test_input = \"He loves and hates the mountains and the beach\"\n",
    "    print(\"\\033[31m\" + \"Original:\" + \"\\033[0m\", test_input)\n",
    "    print(segment_sentences(test_input, False, False))\n",
    "    test_input = \"The big brown fox loves guitar but smashes every guitar he sees\"\n",
    "    print(\"\\033[31m\" + \"Original:\" + \"\\033[0m\", test_input)\n",
    "    print(segment_sentences(test_input, False, False))\n",
    "    test_input = \"Sally and Mr Smith both love ice cream, popsicles and cake \"\n",
    "    print(\"\\033[31m\" + \"Original:\" + \"\\033[0m\", test_input)\n",
    "    print(segment_sentences(test_input, False, False))\n",
    "    test_input = \"I love the product but not the price\"\n",
    "    print(\"\\033[31m\" + \"Original:\" + \"\\033[0m\", test_input)\n",
    "    print(segment_sentences(test_input, False, False))\n",
    "    test_input = \"I bought this product, which broke in two months, so I threw it away\"\n",
    "    print(\"\\033[31m\" + \"Original:\" + \"\\033[0m\", test_input)\n",
    "    print(segment_sentences(test_input, False, False))\n",
    "    test_input = \"I love this product's fit but not the color\"\n",
    "    print(\"\\033[31m\" + \"Original:\" + \"\\033[0m\", test_input)\n",
    "    print(segment_sentences(test_input, False, False))\n",
    "    test_input = \"The turtle won the race, and the rabbit lost his keys\"\n",
    "    print(\"\\033[31m\" + \"Original:\" + \"\\033[0m\", test_input)\n",
    "    print(segment_sentences(test_input, False, False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_env3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
